{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c84e9c3f-6e8c-4979-bba0-2829c24abcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2744146f-f340-473f-ab9a-f7a515bbf110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d427a2-c5a5-4714-8482-e3cfe80168ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Dataset Shape: (50000, 2)\n",
      "Columns: ['review', 'sentiment']\n",
      "\n",
      "First few rows:\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n",
      "None\n",
      "\n",
      "Class Distribution:\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n",
      "DATA QUALITY ASSESSMENT\n",
      "==================================================\n",
      "\n",
      "Missing Values:\n",
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate Records: 418\n",
      "Duplicate Reviews (based on text): 418\n",
      "\n",
      "Sample Review (showing noise):\n",
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ\n",
      "\n",
      "==================================================\n",
      "APPLYING DATA CLEANING\n",
      "==================================================\n",
      "\n",
      "Cleaning text data...\n",
      "\n",
      "Before Cleaning:\n",
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Tru\n",
      "\n",
      "After Cleaning:\n",
      "one of the other reviewers has mentioned that after watching just oz episode you ll be hooked they are right as this is exactly what happened with me the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not \n",
      "\n",
      "Removing duplicate records...\n",
      "Records removed: 423\n",
      "Dataset shape after removing duplicates: (49577, 3)\n",
      "APPLYING DATA REDUCTION (STRATIFIED SAMPLING)\n",
      "==================================================\n",
      "\n",
      "Original dataset size: 49577\n",
      "Sample size (20.0%): 9915\n",
      "Original class distribution:\n",
      "sentiment\n",
      "positive    24882\n",
      "negative    24695\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class proportions:\n",
      "sentiment\n",
      "positive    0.501886\n",
      "negative    0.498114\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Sampled dataset shape: (9914, 3)\n",
      "\n",
      "Sampled class distribution:\n",
      "sentiment\n",
      "positive    4976\n",
      "negative    4938\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original proportions:\n",
      "sentiment\n",
      "positive    0.501886\n",
      "negative    0.498114\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Sampled proportions:\n",
      "sentiment\n",
      "positive    0.501916\n",
      "negative    0.498084\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "==================================================\n",
      "SAVING PREPROCESSED DATA\n",
      "==================================================\n",
      "\n",
      "Cleaned full dataset saved as 'IMDB_cleaned_full.csv'\n",
      "==================================================\n",
      "Original dataset size: 50,000 records\n",
      "After duplicate removal: 49577 records\n",
      "Sampled dataset size: 9914 records\n",
      "Data reduction: 80.00%\n",
      "\n",
      "Preprocessing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Display initial dataset information \n",
    "print(f\"Initial Dataset Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\") \n",
    "print(f\"\\nFirst few rows:\") \n",
    "print(df.head()) \n",
    "print(f\"\\nDataset Info:\")\n",
    "print(df.info()) \n",
    "print(f\"\\nClass Distribution:\") \n",
    "print(df['sentiment'].value_counts())\n",
    "\n",
    "# Check for missing values print(\"\\n\" + \"=\"*50) \n",
    "print(\"DATA QUALITY ASSESSMENT\") \n",
    "print(\"=\"*50) \n",
    "print(f\"\\nMissing Values:\") \n",
    "print(df.isnull().sum())\n",
    "# Check for duplicate records \n",
    "print(f\"\\nDuplicate Records: {df.duplicated().sum()}\") \n",
    "print(f\"Duplicate Reviews (based on text): {df.duplicated(subset=['review']).sum()}\")\n",
    "# Display sample noisy data\n",
    "print(f\"\\nSample Review (showing noise):\")\n",
    "print(df['review'].iloc[0][:500])  # First 500 characters\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# DATA CLEANING - TECHNIQUE 1 \n",
    "# ============================================\n",
    "def clean_text(text):     \n",
    " \"\"\"  Clean text data by removing noise and normalizing         \n",
    "        Steps:\n",
    "1.\tConvert to lowercase\n",
    "2.\tRemove HTML tags\n",
    "3.\tRemove URLs\n",
    "4.\tRemove special characters and numbers\n",
    "5.\tRemove extra whitespace    \"\"\"    \n",
    " # Convert to lowercase     \n",
    " text = text.lower()         \n",
    " # Remove HTML tags     \n",
    " text = re.sub(r'<.*?>', '', text)         \n",
    " # Remove URLs     \n",
    " text = re.sub(r'http\\S+|www\\S+', '', text)     \n",
    " # Remove special characters and numbers, keep only letters and spaces     \n",
    " text = re.sub(r'[^a-z\\s]', ' ', text)         \n",
    " # Remove extra whitespace\n",
    " text = re.sub(r'\\s+', ' ', text).strip()         \n",
    " return text\n",
    "# Apply text cleaning \n",
    "print(\"\\n\" + \"=\"*50) \n",
    "print(\"APPLYING DATA CLEANING\") \n",
    "print(\"=\"*50) \n",
    "print(\"\\nCleaning text data...\") \n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "# Display before and after cleaning \n",
    "print(\"\\nBefore Cleaning:\")\n",
    "print(df['review'].iloc[0][:300]) \n",
    "print(\"\\nAfter Cleaning:\") \n",
    "print(df['cleaned_review'].iloc[0][:300])\n",
    "# Remove duplicate records \n",
    "print(f\"\\nRemoving duplicate records...\") \n",
    "initial_size = len(df) \n",
    "df = df.drop_duplicates(subset=['cleaned_review'], keep='first') \n",
    "final_size = len(df) \n",
    "print(f\"Records removed: {initial_size - final_size}\") \n",
    "print(f\"Dataset shape after removing duplicates: {df.shape}\")\n",
    "# Reset index after removing duplicates \n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# DATA REDUCTION - TECHNIQUE 2: STRATIFIED SAMPLING # ============================================\n",
    "def stratified_sampling(dataframe, sample_size, class_column):    \n",
    "    \"\"\"\n",
    "    Perform stratified sampling to maintain class distribution\n",
    "        Parameters:\n",
    "-\tdataframe: Input DataFrame\n",
    "-\tsample_size: Total number of samples to draw\n",
    "-\tclass_column: Column name containing class labels\n",
    "        Returns:\n",
    "-\tSampled DataFrame with preserved class distribution\n",
    "    \"\"\"     \n",
    "    # Calculate samples per class (proportional sampling)    \n",
    "    class_counts = dataframe[class_column].value_counts()    \n",
    "    class_proportions = class_counts / len(dataframe)         \n",
    "    print(f\"Original class distribution:\")     \n",
    "    print(class_counts)     \n",
    "    print(f\"\\nClass proportions:\")     \n",
    "    print(class_proportions)         \n",
    "    # Sample from each class proportionally    \n",
    "    sampled_dfs = []     \n",
    "    for class_label, proportion in class_proportions.items():         \n",
    "        class_sample_size = int(sample_size * proportion)         \n",
    "        class_df = dataframe[dataframe[class_column] == class_label]         \n",
    "        class_sample = class_df.sample(n=class_sample_size, random_state=42)        \n",
    "        sampled_dfs.append(class_sample)         \n",
    "\n",
    "    # Combine samples from all classes     \n",
    "    sampled_df = pd.concat(sampled_dfs, ignore_index=True)         \n",
    "\n",
    "    # Shuffle the combined sample     \n",
    "    sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)         \n",
    "    return sampled_df\n",
    "# Apply stratified sampling print(\"\\n\" + \"=\"*50) \n",
    "print(\"APPLYING DATA REDUCTION (STRATIFIED SAMPLING)\") \n",
    "print(\"=\"*50)\n",
    "# Define sample size (e.g., 20% of original data) \n",
    "sample_percentage = 0.2 \n",
    "sample_size = int(len(df) * sample_percentage)\n",
    "print(f\"\\nOriginal dataset size: {len(df)}\")\n",
    "print(f\"Sample size ({sample_percentage*100}%): {sample_size}\")\n",
    "# Perform stratified sampling \n",
    "df_sampled = stratified_sampling(df, sample_size, 'sentiment')\n",
    "print(f\"\\nSampled dataset shape: {df_sampled.shape}\")\n",
    "print(f\"\\nSampled class distribution:\")\n",
    "print(df_sampled['sentiment'].value_counts())\n",
    "# Verify proportions are maintained \n",
    "print(f\"\\nOriginal proportions:\") \n",
    "print(df['sentiment'].value_counts(normalize=True)) \n",
    "print(f\"\\nSampled proportions:\")\n",
    "print(df_sampled['sentiment'].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "# Save cleaned full dataset \n",
    "print(\"\\n\" + \"=\"*50) \n",
    "print(\"SAVING PREPROCESSED DATA\") \n",
    "print(\"=\"*50) \n",
    "df.to_csv('IMDB_cleaned_full.csv', index=False)\n",
    "print(\"\\nCleaned full dataset saved as 'IMDB_cleaned_full.csv'\")\n",
    "# Save sampled dataset df_sampled.to_csv('IMDB_cleaned_sampled.csv', index=False) print(\"Sampled dataset saved as 'IMDB_cleaned_sampled.csv'\")\n",
    "# Summary statistics print(\"\\n\" + \"=\"*50) print(\"PREPROCESSING SUMMARY\") \n",
    "print(\"=\"*50) \n",
    "print(f\"Original dataset size: 50,000 records\") \n",
    "print(f\"After duplicate removal: {len(df)} records\") \n",
    "print(f\"Sampled dataset size: {len(df_sampled)} records\") \n",
    "print(f\"Data reduction: {(1 - len(df_sampled)/len(df))*100:.2f}%\") \n",
    "print(\"\\nPreprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53982669-3f30-4470-bba1-0fc824020042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49577, 3)\n",
      "['review', 'sentiment', 'cleaned_review']\n",
      "one of the other reviewers has mentioned that after watching just oz episode you ll be hooked they are right as this is exactly what happened with me the first thing that struck me about oz was its br\n",
      "Missing cleaned_review: 0\n",
      "  obj_id sentiment                                     cleaned_review\n",
      "0    R01  negative  this is the most disturbing film i have ever s...\n",
      "1    R02  negative  this movie was probably one of the worst movie...\n",
      "2    R03  negative  this is by far the worst non english horror mo...\n",
      "3    R04  negative  blue desert may have had the potential to be e...\n",
      "4    R05  negative  the writer director of this film obviously doe...\n",
      "5    R06  positive  not since the simpsons made it s debut has the...\n",
      "6    R07  positive  finding the premise intriguing and reading the...\n",
      "7    R08  positive  real cool smart movie i loved sheedy s colors ...\n",
      "8    R09  positive  why all the negative reviews you didn t expect...\n",
      "9    R10  positive  this is a good film for die hard chucky fans o...\n",
      "Class distribution: {'negative': 5, 'positive': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smera\\AppData\\Local\\Temp\\ipykernel_14228\\507387047.py:14: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=5, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "#Load pre-cleaned dataset\n",
    "df = pd.read_csv('IMDB_cleaned_full.csv')\n",
    "print(df.shape)           #(49577, 3) -> review | sentiment | cleaned_review\n",
    "print(df.columns.tolist())\n",
    "\n",
    "#Verify the cleaned_review column exists and is clean\n",
    "print(df['cleaned_review'].iloc[0][:200])\n",
    "print(\"Missing cleaned_review:\", df['cleaned_review'].isnull().sum())\n",
    "\n",
    "#SAMPLING: 5 positive + 5 negative = 10 objects\n",
    "np.random.seed(42)\n",
    "df_t5 = (\n",
    "    df.groupby('sentiment', group_keys=False)\n",
    "      .apply(lambda x: x.sample(n=5, random_state=42))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "df_t5['obj_id'] = [f'R{i+1:02d}' for i in range(10)]\n",
    "\n",
    "print(df_t5[['obj_id', 'sentiment', 'cleaned_review']])\n",
    "print(\"Class distribution:\", df_t5['sentiment'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "336c03ce-a154-4efb-abf6-ff5f066de4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "── RAW review ──\n",
      "This is the most disturbing film I have ever seen. It makes \"Requiem for a Dream\" look like a Disney film. Although, technically, it is reasonably well made, acting, cinematography, music, directing, etc., are good. However, the concluding gang rape scene is the most appalling and violent thing I ha\n",
      "\n",
      "── CLEANED review ──\n",
      "this is the most disturbing film i have ever seen it makes requiem for a dream look like a disney film although technically it is reasonably well made acting cinematography music directing etc are good however the concluding gang rape scene is the most appalling and violent thing i have ever seen an\n",
      "\n",
      "Reviews still containing HTML tags: 0\n",
      "Reviews with uppercase chars: 0\n"
     ]
    }
   ],
   "source": [
    "#Confirm cleaning\n",
    "sample_idx = 0\n",
    "\n",
    "print(\"── RAW review ──\")\n",
    "print(df_t5['review'].iloc[sample_idx][:300])\n",
    "\n",
    "print(\"\\n── CLEANED review ──\")\n",
    "print(df_t5['cleaned_review'].iloc[sample_idx][:300])\n",
    "\n",
    "import re\n",
    "has_html = df_t5['cleaned_review'].str.contains(r'<[^>]+>', regex=True)\n",
    "print(f\"\\nReviews still containing HTML tags: {has_html.sum()}\")\n",
    "\n",
    "has_upper = df_t5['cleaned_review'].str.contains(r'[A-Z]')\n",
    "print(f\"Reviews with uppercase chars: {has_upper.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08fa77ad-061a-47ad-8889-fc3af87ef214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (10, 1511)\n",
      "Vocabulary size: 1511\n",
      "\n",
      "Pairwise Cosine Similarity Matrix:\n",
      "        R01     R02     R03     R04     R05     R06     R07     R08     R09  \\\n",
      "R01  1.0000  0.0357  0.0341  0.0103  0.0274  0.0167  0.0120  0.0359  0.0257   \n",
      "R02  0.0357  1.0000  0.0473  0.0261  0.0294  0.0444  0.0288  0.0113  0.0400   \n",
      "R03  0.0341  0.0473  1.0000  0.0121  0.0201  0.0224  0.0029  0.0052  0.0272   \n",
      "R04  0.0103  0.0261  0.0121  1.0000  0.0137  0.0328  0.0050  0.0028  0.0188   \n",
      "R05  0.0274  0.0294  0.0201  0.0137  1.0000  0.0240  0.0193  0.0027  0.0043   \n",
      "R06  0.0167  0.0444  0.0224  0.0328  0.0240  1.0000  0.0201  0.0075  0.0217   \n",
      "R07  0.0120  0.0288  0.0029  0.0050  0.0193  0.0201  1.0000  0.0021  0.0213   \n",
      "R08  0.0359  0.0113  0.0052  0.0028  0.0027  0.0075  0.0021  1.0000  0.0259   \n",
      "R09  0.0257  0.0400  0.0272  0.0188  0.0043  0.0217  0.0213  0.0259  1.0000   \n",
      "R10  0.0387  0.0553  0.0340  0.0168  0.0420  0.0337  0.0173  0.0127  0.0388   \n",
      "\n",
      "        R10  \n",
      "R01  0.0387  \n",
      "R02  0.0553  \n",
      "R03  0.0340  \n",
      "R04  0.0168  \n",
      "R05  0.0420  \n",
      "R06  0.0337  \n",
      "R07  0.0173  \n",
      "R08  0.0127  \n",
      "R09  0.0388  \n",
      "R10  1.0000  \n",
      "\n",
      "── TOP 10 PAIRS ──\n",
      "   pair    score   sent_A   sent_B  same_cls\n",
      "R02–R10 0.055297 negative positive     False\n",
      "R02–R03 0.047275 negative negative      True\n",
      "R02–R06 0.044419 negative positive     False\n",
      "R05–R10 0.042003 negative positive     False\n",
      "R02–R09 0.039977 negative positive     False\n",
      "R09–R10 0.038759 positive positive      True\n",
      "R01–R10 0.038747 negative positive     False\n",
      "R01–R08 0.035891 negative positive     False\n",
      "R01–R02 0.035739 negative negative      True\n",
      "R01–R03 0.034125 negative negative      True\n",
      "\n",
      "MAX SIMILARITY: R02–R10  →  0.055297\n",
      "  Sentiment A: negative  |  Sentiment B: positive\n",
      "  Same sentiment class: False\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import itertools\n",
    "\n",
    "#TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features  = 5000,         #top 5000 terms by corpus TF-IDF weight\n",
    "    ngram_range   = (1, 2),        #unigrams + bigrams (\"good film\", \"bad acting\")\n",
    "    sublinear_tf  = True,          #1+log(TF) - handles long-review dominance\n",
    "    stop_words    = 'english',     #sklearn built-in stops (the, is, a, etc.)\n",
    "    min_df        = 1,             #must appear in ≥1 doc (fine for n=10)\n",
    "    max_df        = 0.85,          #ignore terms in >85% of docs (too generic)\n",
    "    strip_accents = 'unicode',\n",
    "    analyzer      = 'word'\n",
    ")\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(df_t5['cleaned_review'])\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "#Full 10×10 pairwise cosine similarity matrix\n",
    "sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "ids = df_t5['obj_id'].tolist()\n",
    "sim_df = pd.DataFrame(sim_matrix, index=ids, columns=ids)\n",
    "\n",
    "print(\"\\nPairwise Cosine Similarity Matrix:\")\n",
    "print(sim_df.round(4))\n",
    "\n",
    "#Extract all C(10,2) = 45 unique pairs\n",
    "pairs = []\n",
    "for i, j in itertools.combinations(range(10), 2):\n",
    "    pairs.append({\n",
    "        'pair'    : f\"{ids[i]}–{ids[j]}\",\n",
    "        'score'   : round(sim_matrix[i][j], 6),\n",
    "        'sent_A'  : df_t5.iloc[i]['sentiment'],\n",
    "        'sent_B'  : df_t5.iloc[j]['sentiment'],\n",
    "        'same_cls': df_t5.iloc[i]['sentiment'] == df_t5.iloc[j]['sentiment']\n",
    "    })\n",
    "\n",
    "ranked = pd.DataFrame(pairs).sort_values('score', ascending=False)\n",
    "\n",
    "print(\"\\n── TOP 10 PAIRS ──\")\n",
    "print(ranked.head(10).to_string(index=False))\n",
    "\n",
    "#Find max pair\n",
    "top = ranked.iloc[0]\n",
    "print(f\"\\nMAX SIMILARITY: {top['pair']}  →  {top['score']:.6f}\")\n",
    "print(f\"  Sentiment A: {top['sent_A']}  |  Sentiment B: {top['sent_B']}\")\n",
    "print(f\"  Same sentiment class: {top['same_cls']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c085687-0e78-4003-8d56-99fdffcc8941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair: R02 (negative) ↔ R10 (positive)\n",
      "Score: 0.055297\n",
      "\n",
      "── cleaned_review A (first 300 chars) ──\n",
      "this movie was probably one of the worst movies i ve seen in a very long time a friend of mine grabbed it off the shelf at the video rental store and all but forced me to watch it an action we both deeply regret ehh where to start the writing the acting the quality all of it sucked quite possibly so\n",
      "\n",
      "── cleaned_review B (first 300 chars) ──\n",
      "this is a good film for die hard chucky fans okay i m sure it s not as good as what the child s play movie were like but this can get really funny and enjoyable chucky s laughs are hilarious spoilers now not one doll but two meaning double the impact jennifer tilly played the part really well and de\n",
      "\n",
      "── Top 15 Shared TF-IDF Terms ──\n",
      "Term                             A        B   Shared\n",
      "movies                      0.0867   0.1134   0.0867\n",
      "movie                       0.0643   0.0839   0.0643\n",
      "good                        0.0622   0.1009   0.0622\n",
      "like                        0.0692   0.0535   0.0535\n",
      "overall                     0.0472   0.0766   0.0472\n",
      "start                       0.0472   0.0766   0.0472\n",
      "funny                       0.0472   0.0766   0.0472\n",
      "ve seen                     0.0413   0.0670   0.0413\n",
      "played                      0.0413   0.0670   0.0413\n",
      "got                         0.0413   0.0670   0.0413\n",
      "course                      0.0413   0.0670   0.0413\n",
      "ve                          0.0367   0.0596   0.0367\n",
      "seen                        0.0330   0.0906   0.0330\n",
      "film                        0.0330   0.0906   0.0330\n",
      "\n",
      "Euclidean distance (TF-IDF space): 1.3746\n",
      "Cosine similarity:                 0.0553\n"
     ]
    }
   ],
   "source": [
    "#Pull max pair and display shared vocabulary\n",
    "top_row  = ranked.iloc[0]\n",
    "id_a, id_b = top_row['pair'].split('–')\n",
    "idx_a = df_t5[df_t5['obj_id'] == id_a].index[0]\n",
    "idx_b = df_t5[df_t5['obj_id'] == id_b].index[0]\n",
    "\n",
    "row_a = df_t5.iloc[idx_a]\n",
    "row_b = df_t5.iloc[idx_b]\n",
    "\n",
    "print(f\"Pair: {id_a} ({row_a.sentiment}) ↔ {id_b} ({row_b.sentiment})\")\n",
    "print(f\"Score: {top_row['score']:.6f}\\n\")\n",
    "\n",
    "print(\"── cleaned_review A (first 300 chars) ──\")\n",
    "print(row_a['cleaned_review'][:300])\n",
    "print(\"\\n── cleaned_review B (first 300 chars) ──\")\n",
    "print(row_b['cleaned_review'][:300])\n",
    "\n",
    "#Shared TF-IDF terms\n",
    "feat   = vectorizer.get_feature_names_out()\n",
    "vec_a  = tfidf_matrix[idx_a].toarray()[0]\n",
    "vec_b  = tfidf_matrix[idx_b].toarray()[0]\n",
    "shared = np.minimum(vec_a, vec_b)\n",
    "\n",
    "#Top 15 shared terms by shared TF-IDF weight\n",
    "top_idx = shared.argsort()[::-1][:15]\n",
    "print(\"\\n── Top 15 Shared TF-IDF Terms ──\")\n",
    "print(f\"{'Term':<25} {'A':>8} {'B':>8} {'Shared':>8}\")\n",
    "for i in top_idx:\n",
    "    if shared[i] > 0:\n",
    "        print(f\"{feat[i]:<25} {vec_a[i]:>8.4f} {vec_b[i]:>8.4f} {shared[i]:>8.4f}\")\n",
    "\n",
    "#Euclidean distance (complementary check)\n",
    "euc = np.linalg.norm(vec_a - vec_b)\n",
    "print(f\"\\nEuclidean distance (TF-IDF space): {euc:.4f}\")\n",
    "print(f\"Cosine similarity:                 {top_row['score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
